{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeevkumar/anaconda/lib/python3.4/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import hdf5_getters\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build path dictionary\n",
    "path_dict_song = {} \n",
    "basedir = \"../MillionSongSubset/data/\" \n",
    "ext = \".h5\"\n",
    "for root, dirs, files in os.walk(basedir):\n",
    "    for f in files:\n",
    "        path_track = os.path.join(root,f)\n",
    "        track_ID = f.split('.')[0]\n",
    "        path_dict_song[track_ID] = path_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build song to track dic:\n",
    "count = 0\n",
    "song_track_dic = {}\n",
    "with open('taste_profile_song_to_tracks.txt') as f:\n",
    "    for line in f:\n",
    "#         print(line)\n",
    "        count += 1\n",
    "        if count < 0:\n",
    "            break\n",
    "        else: \n",
    "            valTuple = line.split('\\t')\n",
    "            if len(valTuple) > 1:\n",
    "                songID = valTuple[0]\n",
    "                trackID = valTuple[1].split('\\n')[0]\n",
    "                song_track_dic[songID] = trackID\n",
    "# We have total 3 million song track data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songCSV = pd.read_csv('SongCSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# userID = 'b80344d063b5ccb3212f76538f3d9e43d87dca9e'\n",
    "# outfile = userID + '_profile.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't execute this as data already saved\n",
    "# # traverse all users.. \n",
    "# count = 0\n",
    "# user_song_dic = {}\n",
    "# with open('train_triplets.txt') as f: \n",
    "#         for line in f:\n",
    "#             count += 1\n",
    "#             if count > 100000:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 userID = line.split('\\t')[0]\n",
    "#                 songID = line.split('\\t')[1]\n",
    "#                 if songID in song_track_dic:\n",
    "#                     trackID = song_track_dic[songID]\n",
    "#                     if trackID in path_dict_song:\n",
    "#                         trackPath = path_dict_song[trackID]\n",
    "#                         if userID not in user_song_dic:\n",
    "#                             user_song_dic[userID] = [trackPath]\n",
    "#                         else:\n",
    "#                             track_list = user_song_dic[userID]\n",
    "#                             track_list.append(trackPath)\n",
    "#                             user_song_dic[userID] = track_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rebuild_user_song_dic = {}\n",
    "count = 0\n",
    "with open('user_songPath.txt') as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        if count < 0:\n",
    "            break\n",
    "        line = line.split('\\n')[0]\n",
    "        list_item = line.split('\\t')\n",
    "        userID = list_item[0]\n",
    "        pathLst = [list_item[i] for i in range(1, len(list_item)-1)]\n",
    "        rebuild_user_song_dic[userID] = pathLst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../MillionSongSubset/data/A/V/V/TRAVVZW12903CCEEE7.h5',\n",
       " '../MillionSongSubset/data/B/D/I/TRBDIOP128EF35F99C.h5',\n",
       " '../MillionSongSubset/data/A/N/D/TRANDEK128F4259EBC.h5',\n",
       " '../MillionSongSubset/data/A/A/K/TRAAKDG128F42A0ECB.h5',\n",
       " '../MillionSongSubset/data/A/S/P/TRASPAS128E078DFB3.h5',\n",
       " '../MillionSongSubset/data/A/F/W/TRAFWEV128F428D391.h5',\n",
       " '../MillionSongSubset/data/B/H/L/TRBHLDQ128F423EF10.h5',\n",
       " '../MillionSongSubset/data/B/D/Q/TRBDQVN128F425CF6D.h5',\n",
       " '../MillionSongSubset/data/B/I/F/TRBIFGK128F92F23A2.h5',\n",
       " '../MillionSongSubset/data/A/N/B/TRANBUW128F933C645.h5',\n",
       " '../MillionSongSubset/data/A/P/T/TRAPTOE128F4272000.h5']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebuild_user_song_dic['9215650e56636976a17e28c623d0830833f970f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(rebuild_user_song_dic)\n",
    "# filter top 10 users with path > 10\n",
    "count = 0\n",
    "top_10_users = []\n",
    "for k,v in rebuild_user_song_dic.items():\n",
    "    if count > 10:\n",
    "        break\n",
    "    if len(v) >= 10:\n",
    "        count += 1\n",
    "        top_10_users.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['943a4a321dbb95dc3613125710076503dd8a5813',\n",
       " '28db9ab0eede05baf69dbb815b0979741fc84b0d',\n",
       " '043d81932e75d5749ed5758d6420506e7bc457a5',\n",
       " '4540b216303f9cab3f8b95afae8d8d4198a0099b',\n",
       " '1154226d04db38bc3b175cb7673189916a06ffcf',\n",
       " '35cc3c27fefde06cd9450697c7fb948219f5b630',\n",
       " '3cac2b1fb844c7c595b8febd30a352dc5780308f',\n",
       " '2e259fa6ebcc8b4df0ee93b8a9b94f3dce8b4272',\n",
       " '72565ddc0b3d54ecd4c079e4b7510167e9142b38',\n",
       " '955e3598ffde094e71a3a0bac5a2635bd281287e',\n",
       " '10d7dead6cfbb76453a9646d23bd2c1c9ba79044']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build FV for each of the 10 users and dump into csv\n",
    "songH5File = None\n",
    "for user in top_10_users:\n",
    "    # initialize\n",
    "    # ....\n",
    "    lst_songID = []\n",
    "    lst_artistFamiliarity = []\n",
    "    lst_artistHottness = []\n",
    "    lst_artistName = []\n",
    "    lst_duration = []\n",
    "    lst_dancebility = []\n",
    "    lst_endOfFadeIn = []\n",
    "    lst_energy = []\n",
    "    lst_loudness = []\n",
    "    lst_songHotness = []\n",
    "    lst_tempo = []\n",
    "    lst_songTitle = []\n",
    "    # ....\n",
    "    \n",
    "    trackPaths = rebuild_user_song_dic[user]\n",
    "    for track in trackPaths:\n",
    "        # read the .h5 file\n",
    "        songH5File = hdf5_getters.open_h5_file_read(track)\n",
    "        \n",
    "        songID = str(hdf5_getters.get_song_id(songH5File))\n",
    "        lst_songID.append(songID)\n",
    "        \n",
    "        artistFamiliarity = hdf5_getters.get_artist_familiarity(songH5File)\n",
    "        lst_artistFamiliarity.append(artistFamiliarity)\n",
    "        \n",
    "        artistHottness = hdf5_getters.get_artist_hotttnesss(songH5File)\n",
    "        lst_artistHottness.append(artistHottness)\n",
    "        \n",
    "        artistName = hdf5_getters.get_artist_name(songH5File)\n",
    "        lst_artistName.append(artistName)\n",
    "        \n",
    "        duration = hdf5_getters.get_duration(songH5File)\n",
    "        lst_duration.append(duration)\n",
    "        \n",
    "        dancebility = hdf5_getters.get_danceability(songH5File)\n",
    "        lst_dancebility.append(dancebility)\n",
    "        \n",
    "        endOfFadeIn = hdf5_getters.get_end_of_fade_in(songH5File)\n",
    "        lst_endOfFadeIn.append(endOfFadeIn)\n",
    "        \n",
    "        energy = hdf5_getters.get_energy(songH5File)\n",
    "        lst_energy.append(energy)\n",
    "        \n",
    "        loudness = hdf5_getters.get_loudness(songH5File)\n",
    "        lst_loudness.append(loudness)\n",
    "        \n",
    "        songHotness = hdf5_getters.get_song_hotttnesss(songH5File)\n",
    "        lst_songHotness.append(songHotness)\n",
    "        \n",
    "        tempo = hdf5_getters.get_tempo(songH5File)\n",
    "        lst_tempo.append(tempo)\n",
    "        \n",
    "        songTitle = hdf5_getters.get_title(songH5File)\n",
    "        lst_songTitle.append(songTitle)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # construct dataframe for this user\n",
    "    userFV = pd.DataFrame({'songID': lst_songID, 'artistFamiliarity': lst_artistFamiliarity, 'artistHottness': lst_artistHottness,\n",
    "                         'artistName': lst_artistName, 'duration': lst_duration, 'dancebility': lst_dancebility,\n",
    "                         'endOfFadeIn': lst_endOfFadeIn, 'energy': lst_energy, 'loudness': lst_loudness, 'songHotness': lst_songHotness,\n",
    "                         'tempo': lst_tempo, 'songTitle': lst_songTitle})\n",
    "    outputFile = user + '_FV.csv'\n",
    "    userFV.to_csv(outputFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't execute this\n",
    "# # dump user_song_dic\n",
    "# with open('user_songPath.txt', 'w') as ofile:\n",
    "#     output_str = ''\n",
    "#     for k,v in user_song_dic.items():\n",
    "#         output_str += k + '\\t'\n",
    "#         for item in v:\n",
    "#             output_str += item + '\\t'\n",
    "#         output_str += '\\n'\n",
    "#     ofile.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "# from pyspark import SparkContext\n",
    "# import os\n",
    "# os.environ[\"SPARK_HOME\"] = \"/Users/rajeevkumar/spark-2.0.1-bin-hadoop2.7\"\n",
    "\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# # Load training data\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "# training = spark.read.format(\"csv\").load(\"043d81932e75d5749ed5758d6420506e7bc457a5_FV.csv\")\n",
    "\n",
    "# lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# # Fit the model\n",
    "# lrModel = lr.fit(training)\n",
    "\n",
    "# # Print the coefficients and intercept for linear regression\n",
    "# print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "# print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "trainX = pd.read_csv('043d81932e75d5749ed5758d6420506e7bc457a5_FV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['artistFamiliarity', 'artistHottness',\n",
    "       'dancebility', 'duration', 'endOfFadeIn', 'energy', 'loudness',\n",
    "       'songHotness', 'tempo']\n",
    "X = trainX[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python_tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
